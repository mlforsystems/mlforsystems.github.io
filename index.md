---
title: Announcement
workshop_name: neurips2023
site_description: Workshop on ML for Systems at NeurIPS 2023, December, New Orleans
mini_site_description: Workshop on ML for Systems at NeurIPS '23, Dec
site_title: ML For Systems
---
<div class="inner clearfix">
	<section class="main-content overview_section">
		<h2>Overview</h2>
        <p>The ML for Systems workshop presents cutting-edge work on ML in computer systems and aims to develop a unified methodology for the field.
        </p>
        <p>Machine Learning (ML) for Systems describes the application of machine learning techniques to problems related to computer systems. By leveraging supervised learning and reinforcement learning (RL) approaches, machine learning can replace longstanding heuristics that currently drive many of these systems. This includes a wide range of topics, including multi-objective tasks such as designing new data structures <sup>1</sup>, integrated circuits <sup>2, 3</sup>, or design verification <sup>20, 21</sup>, as well as implementing control algorithms for applications such as compilers <sup>12, 13, 19</sup>, databases <sup>8</sup>, memory management <sup>9, 10</sup>, or ML frameworks <sup>11</sup>. While the systems community increasingly recognizes the importance of ML in solving a variety of different systems problems <sup>23</sup>, ML for Systems remains an emerging area without widely established best practices, methods and strategies for the application of state-of-the-art machine learning techniques <sup>22</sup>. The goal of this workshop is to provide an interdisciplinary venue for ML and Systems experts to push this boundary and start new directions within the ML for Systems area.
        </p>
        <h3>Workshop Direction</h3>
        <p>
        In previous 6 editions, we showcased specific approaches and frameworks to solve problems, bringing together researchers and practitioners at NeurIPS from both the ML and systems communities. While breaking new grounds, we encouraged collaborations and development in a broad range of ML for Systems works, many later published in top-tier conferences <sup>11,13,14,15,16,17,18</sup>. This year, we plan to continue this path while encouraging work in key emerging areas such as Large Language Model (LLM) training and serving, and unifying benchmarks on key problems such as scheduling and compiling through a competition.
        </p>
        <p>Recently, the rise of Large Language Models (LLMs) has presented new opportunities and challenges within the domain of computer systems. Our community is well-positioned to produce science and stimulate discussion for adapting to the new paradigm, especially how LLMs can be used to solve systems problems, and using ML to address systems issues that emerge from LLM training and serving. Additionally, as the field matures, we emphasize on keeping the research open, and the science reproducible. To that end, we are supplementing our main program with a competition track to crystallize the field’s progress.
        </p>
        <h3>Workshop Goals </h3>
        <p>NeurIPS provides a unique opportunity to bring together systems researchers and researchers from other sub-areas of ML who had not previously considered applying their techniques in a computer systems context. We see the goal of our workshop as solving the following two objectives:
        <ul>
            <li>Opening up connections between research areas that were not previously considered, connecting the ML and Systems communities, growing the scope of ML for Systems work and unlocking new research opportunities.</li>
            <li>Developing best practices, methodologies and benchmarks for the ML for Systems field.</li>
        </ul>
        </p>
    <h2>Call for Papers</h2>
    <p>We invite researchers to submit relevant papers through our <a href="/call_for_papers.html">call for papers</a>. Our program include contributed speakers and poster sessions from selected works.</p>
    <h2>Competition Track (<b>NEW, Coming Soon</b>)</h2>
    <p>We introduce an auxiliary competition track to showcase state-of-the-art capabilities under open benchmarks. The first iteration serves as an initiative to encourage industrial sharing ML for systems data, with the tentative topic on <b>predicting program runtime from XLA graphs</b>.</p>
    <p> If you would like to be notified when the competition is up, please join https://groups.google.com/g/tpu_graphs_competition.</p>
    <p> The top winners have the opportunities to share their technical reports to receive sponsor prizes. To build commonalities on the topic of LLMs interacting with computational systems, we specifically include seminal talks on emerging trends on training and serving LLMs from seasoned researchers and practitioners as a part of our invited speakers. Our call for papers also includes topics at the intersection of Systems and LLMs.
    </p>
    <ul class="footnotes">
		<li><sup>1</sup> <a href="https://arxiv.org/abs/1706.04972">The case for learned index structures</a></li>
        <li><sup>2</sup> <a href="https://openreview.net/forum?id=Hkc-TeZ0W">Learning to design circuits</a></li>
		<li><sup>3</sup> <a href="https://arxiv.org/abs/1712.01208">A graph placement methodology for fast chip design</a></li>
		<li><sup>4</sup> <a href="https://arxiv.org/abs/1803.02329">Learning Memory Access Patterns</a></li>
		<li><sup>5</sup> <a href="https://ieeexplore.ieee.org/document/8091247/?reload=true">End to End Deep Learning of Optimization Heuristics</a></li>
		<li><sup>6</sup> <a href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/">Deepmind AI Reduces Google Data Centre Cooling Bill</a></li>
		<li><sup>7</sup> <a href="https://www.youtube.com/watch?v=YhNl468S8CI">Bayesian optimization for tuning the JVM</a></li>
		<li><sup>8</sup> <a href="https://arxiv.org/abs/1711.11165">Safe Exploration for Identifying Linear Systems via Robust Optimization</a></li>
        <li><sup>9</sup><a href="https://arxiv.org/abs/1803.02329">Learning Memory Access Patterns</a></li>
        <li><sup>10</sup><a href="https://research.google/pubs/pub49008/">Learning-based memory allocation for C++ server workloads</a></li>
        <li><sup>11</sup><a href="https://arxiv.org/abs/1906.08879">Placeto: Learning Generalizable Device Placement Algorithms for Distributed Machine Learning</a></li>
        <li><sup>12</sup><a href="https://arxiv.org/abs/1805.03441">Machine learning in compiler optimization</a></li>
        <li><sup>13</sup><a href="https://arxiv.org/abs/1805.08166">Learning to Optimize Tensor Programs</a></li>
        <li><sup>14</sup><a href="https://arxiv.org/abs/1810.01963">Learning scheduling algorithms for data processing clusters</a></li>
        <li><sup>15</sup><a href="https://arxiv.org/abs/1811.01704">Releq: An automatic reinforcement learning approach for deep quantization of neural networks</a></li>
        <li><sup>16</sup><a href="https://arxiv.org/abs/1808.07412">Ithemal: Accurate, portable and fast basic block throughput estimation using deep neural networks</a></li>
        <li><sup>17</sup><a href="https://arxiv.org/abs/2104.04955">A Deep Learning Based Cost Model for Automatic Code Optimization</a></li>
        <li><sup>18</sup><a href="https://dl.acm.org/doi/10.1145/3439706.3447045">The Law of Attraction: Affinity-Aware Placement Optimization using Graph Neural Networks</a></li>
        <li><sup>19</sup><a href="https://arxiv.org/abs/2011.14486">Value Learning For Throughput Optimization Of Deep Neural Networks</a></li>
        <li><sup>20</sup><a href="https://dvcon-proceedings.org/wp-content/uploads/Adaptive-Test-Generation-for-Fast-Functional-Coverage-Closure.pdf">Adaptive Test Generation for Fast Functional Coverage Closure</a></li>
        <li><sup>21</sup><a href="https://dvcon-proceedings.org/wp-content/uploads/Test-Parameter-Tuning-with-Blackbox-Optimization-A-Simple-Yet-Effective-Way-to-Improve-Coverage-1.pdf">Test Parameter Tuning with Blackbox Optimization: A Simple Yet Effective Way to Improve Coverage</a></li>
        <li><sup>22</sup><a href="https://ieeexplore.ieee.org/document/9153088">A Taxonomy of ML for Systems Problems</a></li>
        <li><sup>23</sup><a href="https://www.sigarch.org/5-guidelines-for-research-in-ml-for-systems/">5 Guidelines for Research in ML for Systems”, Computer Architecture Today (Blog)</a></li>
        <li><sup>24</sup><a href="https://arxiv.org/abs/2201.00561">Zero-Shot Cost Models for out-of-the-Box Learned Cost Prediction</a></li>
        <li><sup>25</sup><a href="https://ieeexplore.ieee.org/document/9563030">A flexible approach to autotuning multi-pass machine learning compilers</a></li>
        <li><sup>26</sup><a href="https://blog.google/intl/en-africa/products/explore-get-answers/an-important-next-step-on-our-ai-journey/">An important next step on our AI journey (blog) </a></li>
        <li><sup>27</sup><a href="https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html">PaLM API & MakerSuite: an approachable way to start prototyping and building generative AI applications</a></li>
        <li><sup>28</sup><a href="https://blog.google/products/search/search-labs-ai-announcement">David Gasca. “Help us build the future of Search with Search Labs</a></li>
        <li><sup>29</sup><a href="https://arxiv.org/abs/1910.01500">MLPerf Training Benchmark</a></li>
        <li><sup>30</sup><a href="https://arxiv.org/abs/2012.02328">Mlperf mobile inference benchmark: An industry-standard open-source machine learning benchmark for on-device ai</a></li>
        <li><sup>31</sup><a href="https://arxiv.org/abs/2008.01040">A learned performance model for tensor processing units</a></li>
        <li><sup>32</sup><a href="https://arxiv.org/abs/2305.12322">Learning large graph property prediction via graph segment training.</a></li>
    </ul>
	</section>
</div>
<div class="organizers-section">
	<div class="inner clearfix">
		<section class="main-content">
			<h2>Organizing Committee</h2>
			<ul>
				<li><b>Mimee Xu</b>, NYU, <a href="https://twitter.com/MimeeXu">@MimeeXu</a></li>
                <li><b>Dan Zhang</b>, Google DeepMind, <a href="https://www.linkedin.com/in/danzhang3">LinkedIn</a></li>
                <li><b>Phitchaya Mangpo Phothilimthana</b>, Google DeepMind, <a href="https://www.linkedin.com/in/phitchaya-mangpo-phothilimthana">LinkedIn</a></li>
                <li><b>Beidi Chen</b>, CMU, <a href="https://twitter.com/BeidiChen">@BeidiChen</a></li>
                <li><b>Yawen Wang</b>, Google SystemsResearch, <a href="https://www.linkedin.com/in/yawen-wang-589b38113">@LinkedIn</a></li>
                <li><b>Divya Mahajan</b>, Microsoft Research and Georgia Tech. <a href="https://twitter.com/divyamahajn">@DivyaMahajn</a></li>
			</ul>
            <h2>Competition Committee</h2>
			<ul>
                <li>Bryan Perozzi, Google Research</li>
                <li><b>Phitchaya Mangpo Phothilimthana</b>, Google DeepMind, <a href="https://www.linkedin.com/in/phitchaya-mangpo-phothilimthana">LinkedIn</a></li>
                <li>Sami Abu-el-haija, Google Research</li>
            </ul>
            <h2>Steering Committee</h2>
			<ul>
                <li><b>Martin Maas</b>, Google DeepMind, <a href="https://twitter.com/martin_maas">@martin_maas</a></li>
                <li><b>Jonathan Raiman</b>, NVIDIA, <a href="https://twitter.com/jonathanrraiman">@jonathanrraiman</a></li>
                <li><b>Anna Goldie</b>, Anthropic, <a href="https://twitter.com/annadgoldie">@annadgoldie</a></li>
                <li><b>Azalia Mirhoseini</b>, Anthropic, <a href="https://twitter.com/Azaliamirh">@Azaliamirh</a></li>
				<li><b>Milad Hashemi</b>, Google, <a href="https://twitter.com/miladhash">@miladhash</a></li>
				<li><b>Kevin Swersky</b>, Google, <a href="https://twitter.com/kswersk">@kswersk</a></li>
			</ul>
            <h2>Contact Us</h2>
            <p>
                Contact us at <a href="mailto:mlforsystems@googlegroups.com">mlforsystems@googlegroups.com</a>.
            </p>
		</section>
</div>