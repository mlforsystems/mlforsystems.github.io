---
title: Announcement
site_description: Workshop on ML for Systems at NeurIPS 2025, December 6, San Diego Convention Center, Upper Level Room 5AB
mini_site_description: Workshop on ML for Systems at NeurIPS '25, Dec 6, Upper Level Room 5AB
site_title: ML For Systems
---
<div class="speaker_section">
  <div class="inner clearfix">
    <section class="main-content">
      <h2 id="speakers">Speakers</h2>
	    <div class="speaker-bio" id="azalia_bio">
			<div class="img-holder" style="background-image: url(/assets/images/speakers/azalia_mirhoseini.jpg)"></div>
			<div>
				<h3 class="keynote-speaker">Azalia Mirhoseini</h3>
                <h5 class="keynote-speaker">Keynote Speaker</h5>
                <h3>Self-improving AI and the Future of Computing Systems</h3>
				<p>Azalia Mirhoseini is an Assistant Professor of Computer Science and founder of Scaling Intelligence Lab at Stanford University. Her lab develops scalable and self-improving AI systems and methodologies towards the goal of advancing artificial general intelligence. She has spent several years in industry AI labs, including Google Brain, Anthropic, and Google DeepMind. Her past work includes Mixture-of-Experts (MoE) neural architectures, now commonly used in leading generative AI models; AlphaChip, a pioneering work on deep reinforcement learning for layout optimization used in the design of advanced chips like AI accelerators (TPUs) and data center CPUs; and research on inference-time scaling laws. Her research has been recognized through the MIT Technology Review 35 Under 35 Award, Okawa Foundation Research Award, Best ECE Thesis Award at Rice University, publications in flagship venues such as Nature, and coverage by various media outlets, including MIT Technology Review, IEEE Spectrum, The Verge, The Times, ZDNet, VentureBeat, and WIRED.</p>
			</div>
        </div>
        <div class="speaker-bio">
			<div class="img-holder" style="background-image: url(/assets/images/speakers/ion_stoica.png)"></div>
			<div>
				<h3 class="keynote-speaker">Ion Stoica</h3>
                <h5 class="keynote-speaker">Keynote</h5>
                <h3> How AI is Disrupting Systems Research</h3>
				<p> Ion Stoica is a Professor in the EECS Department and holds the Xu Bao Chancellor Chair at the University of California, Berkeley. He is the Director of the Sky Computing Lab and the Executive Chairman of Databricks and Anyscale. His current research focuses on AI systems and cloud computing, and his work includes numerous open-source projects such as vLLM, SGLang, Chatbot Arena, SkyPilot, Ray, and Apache Spark. He is a Member of the National Academy of Engineering, an Honorary Member of the Romanian Academy, and an ACM Fellow. He has also co-founded several companies, including LMArena (2025), Anyscale (2019), Databricks (2013), and Conviva (2006).
				</p>
			</div>
        </div>
        <div class="speaker-bio">
			<div class="img-holder" style="background-image: url(/assets/images/speakers/hanson_wang.png)"></div>
			<div>
				<h3 class="keynote-speaker">Hanson Wang</h3>
                <h5 class="keynote-speaker">Invited Talk</h5>
                <h3>Coding Agents at Scale with OpenAI Codex</h3>
				<p>Hanson Wang is a research engineer at OpenAI, where he focuses on the Codex models integrated into ChatGPT. With Codex, users can delegate coding tasks to parallel agents working autonomously in the cloud to analyze the codebase and generate pull requests. Hanson worked on training the first codex-1 model launched in May and has been continuously iterating on the model since then. Prior to joining OpenAI, he co-founded a startup building AI analyst agents, and worked on ML infrastructure at Meta. Hanson holds a degree in Computer Science from the University of Waterloo.
				</p>
			</div>
        </div>
        <div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/vinod_grover.jpg)"></div>
				<div>
					<h3 class="keynote-speaker">Vinod Grover</h3>
          <h5 class="keynote-speaker">Invited Talk</h5>
                    <h3>The Essence of CUDA and AI for GPUs</h3>
					<p>Vinod Grover is a Sr. Distinguished Engineer at NVIDIA, where he has worked since 2007. He led the team that created the CUDA C++ language and compiler, helping make GPU computing faster and easier across many fields. Since 2017, he has applied language and compiler ideas to accelerate deep-learning models, leading a small group focused on performance and developer productivity. He also continues to advance GPU architectures and the CUDA programming model. Previously, he held engineering, research, and management roles at Sun Microsystems and Microsoft. He holds a bachelor’s in physics from IIT Delhi and a master’s in computer science from Syracuse University.
					</p>
				</div>
        </div>
            <div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/neeraja_yadwadkar.png)"></div>
				<div>
					<h3 class="keynote-speaker">Neeraja Yadwakar </h3>
          <h5 class="keynote-speaker">Invited Talk</h5>
                    <h3>TBD</h3>
					<p>Neeraja J. Yadwadkar is an assistant professor in the department of ECE at UT Austin. She is a Cloud Computing Systems researcher, with a strong background in Machine Learning (ML). Her works straddle the boundaries of Systems and ML. Specifically, advances in systems, machine learning, and hardware architectures are about to launch a new era in which we can use the entire cloud as a computer. On the other hand, new ML techniques are being developed for solving complex resource management problems in systems. Similarly, systems research is getting influenced by properties of emerging ML algorithms, and evolving hardware architectures.  Bridging these complementary fields, her research focuses on using and developing ML techniques for systems, and building systems for ML.
					</p>
				</div>
        </div>
		   <div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/rahul_arya.png)"></div>
				<div>
					<h3 class="keynote-speaker">Rahul Arya</h3>
          <h5 class="keynote-speaker">Invited Talk</h5>
                    <h3>Advances in LLM Serving Efficiency at Scale</h3>
					<p>Rahul Arya is a research engineer at Google DeepMind contributing to the training and inference performance of Gemini models. He previously worked on the XLA:TPU compiler.
					</p>
				</div>
        </div>
    </section>
</div>
</div>

<div class="inner clearfix">
	<section class="main-content overview_section">
		<h2>What To Expect</h2>
        <p>The ML for Systems workshop presents cutting-edge work on ML in computer systems and aims to develop a unified methodology for the field.
        </p>
        <p>Machine Learning (ML) for Systems describes the application of machine learning techniques to problems related to computer systems. By leveraging supervised learning and reinforcement learning (RL) approaches, machine learning can replace longstanding heuristics that currently drive many of these systems. This includes a wide range of topics, including multi-objective tasks such as designing new data structures <sup><a href="https://arxiv.org/abs/1706.04972">1</a></sup>, integrated circuits <sup><a href="https://openreview.net/forum?id=Hkc-TeZ0W">2</a>, <a href="https://arxiv.org/abs/1712.01208">3</a></sup>, or design verification <sup><a href="https://dvcon-proceedings.org/wp-content/uploads/Adaptive-Test-Generation-for-Fast-Functional-Coverage-Closure.pdf">20</a>, <a href="https://dvcon-proceedings.org/wp-content/uploads/Test-Parameter-Tuning-with-Blackbox-Optimization-A-Simple-Yet-Effective-Way-to-Improve-Coverage-1.pdf">21</a></sup>, as well as implementing control algorithms for applications such as compilers <sup><a href="https://arxiv.org/abs/1805.03441">12</a>, <a href="https://arxiv.org/abs/1805.08166">13</a>, <a href="https://arxiv.org/abs/2011.14486">19</a></sup>, databases <sup><a href="https://arxiv.org/abs/1711.11165">8</a></sup>, memory management <sup><a href="https://arxiv.org/abs/1803.02329">9</a>, <a href="https://research.google/pubs/pub49008/">10</a></sup>, or ML frameworks <sup><a href="https://arxiv.org/abs/1906.08879">11</a></sup>. While the systems community increasingly recognizes the importance of ML in solving a variety of different systems problems <sup><a href="https://www.sigarch.org/5-guidelines-for-research-in-ml-for-systems/">23</a></sup>, ML for Systems remains an emerging area without widely established best practices, methods and strategies for the application of state-of-the-art machine learning techniques <sup><a href="https://ieeexplore.ieee.org/document/9153088">22</a></sup>. The goal of this workshop is to provide an interdisciplinary venue for ML and Systems experts to push this boundary and start new directions within the ML for Systems area.
        </p>
        <h3>Workshop Direction</h3>
        <p>
        In previous 6 editions, we showcased specific approaches and frameworks to solve problems, bringing together researchers and practitioners at NeurIPS from both the ML and systems communities. While breaking new grounds, we encouraged collaborations and development in a broad range of ML for Systems works, many later published in top-tier conferences <sup><a href="https://arxiv.org/abs/1906.08879">11</a>, <a href="https://arxiv.org/abs/1805.08166">13</a>, <a href="https://arxiv.org/abs/1810.01963">14</a>, <a href="https://arxiv.org/abs/1811.01704">15</a>, <a href="https://arxiv.org/abs/1808.07412">16</a>, <a href="https://arxiv.org/abs/2104.04955">17</a>, <a href="https://dl.acm.org/doi/10.1145/3439706.3447045">18</a></sup>. This year, we plan to continue this path while encouraging work in key emerging areas such as Large Language Model (LLM) training and serving, and unifying benchmarks on key problems such as scheduling and compiling through a competition.
        </p>
        <p>Recently, the rise of Large Language Models (LLMs) has presented new opportunities and challenges within the domain of computer systems. Our community is well-positioned to produce science and stimulate discussion for adapting to the new paradigm, especially how LLMs can be used to solve systems problems, and using ML to address systems issues that emerge from LLM training and serving. Additionally, as the field matures, we emphasize on keeping the research open, and the science reproducible. To that end, we are supplementing our main program with a competition track to crystallize the field’s progress.
        </p>
        <h3>Workshop Goals </h3>
        <p>NeurIPS provides a unique opportunity to bring together systems researchers and researchers from other sub-areas of ML who had not previously considered applying their techniques in a computer systems context. We see the goal of our workshop as solving the following two objectives:
        <ul>
            <li>Opening up connections between research areas that were not previously considered, connecting the ML and Systems communities, growing the scope of ML for Systems work and unlocking new research opportunities.</li>
            <li>Developing best practices, methodologies and benchmarks for the ML for Systems field.</li>
        </ul>
        </p>
        <p>To build commonalities on the topic of LLMs interacting with computational systems, we specifically include seminal talks on emerging trends on training and serving LLMs from seasoned researchers and practitioners as a part of our invited speakers. Our call for papers also includes topics at the intersection of Systems and LLMs.
        </p>
        <p>Our program will include a variety of speakers and poster sessions from selected papers. We invite researchers to submit relevant papers through our <a href="/call_for_papers.html">call for papers</a>.</p>
	</section>
</div>
<div class="organizers-section">
	<div class="inner clearfix">
		<section class="main-content">
			<h2>Organizing Committee</h2>
			<ul>
				<li><b>Mimee Xu</b>, NYU, <a href="https://twitter.com/MimeeXu">@MimeeXu</a></li>
                <li><b>Dan Zhang</b>, Google DeepMind, <a href="https://www.linkedin.com/in/danzhang3">LinkedIn</a></li>
                <li><b>Phitchaya Mangpo Phothilimthana</b>, OpenAI, <a href="https://www.linkedin.com/in/phitchaya-mangpo-phothilimthana">LinkedIn</a></li>
                <li><b>Divya Mahajan</b>, Georgia Tech, <a href="https://twitter.com/divyamahajn">@DivyaMahajn</a></li>
                <li><b>Haoran Qiu</b>, Microsoft Azure Research, <a href="https://www.linkedin.com/in/jamesqhr/">LinkedIn</a></li>
                <li><b>Patrick Musau</b>, Google, <a href="https://www.linkedin.com/in/musaup/">LinkedIn</a></li>
			</ul>
            <!-- <h2>Steering Committee</h2>
			<ul>
                <li><b>Martin Maas</b>, Google DeepMind, <a href="https://twitter.com/martin_maas">@martin_maas</a></li>
                <li><b>Jonathan Raiman</b>, NVIDIA, <a href="https://twitter.com/jonathanrraiman">@jonathanrraiman</a></li>
                <li><b>Anna Goldie</b>, Google DeepMind, <a href="https://twitter.com/annadgoldie">@annadgoldie</a></li>
                <li><b>Azalia Mirhoseini</b>, Stanford, <a href="https://twitter.com/Azaliamirh">@Azaliamirh</a></li>
				<li><b>Milad Hashemi</b>, Google, <a href="https://twitter.com/miladhash">@miladhash</a></li>
				<li><b>Kevin Swersky</b>, Google, <a href="https://twitter.com/kswersk">@kswersk</a></li>
			</ul> -->
            <h2>Contact Us</h2>
            <p>
                Contact us at <a href="mailto:mlforsystems@googlegroups.com">mlforsystems@googlegroups.com</a>.
            </p>
		</section>
</div>
