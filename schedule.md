---
title: Schedule
workshop_name: neurips2023
site_description: Workshop on ML for Systems at NeurIPS 2023
mini_site_description: Workshop on ML for Systems at NeurIPS 2023
site_title: ML For Systems
---

<div class="speaker_section">
  <div class="inner clearfix">
    <section class="main-content">
      <h2 id="speakers">Speakers</h2>
	    <div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/chris_lattner.jpeg)"></div>
				<div>
					<h3 class="keynote-speaker">Programming Languages Challenges in Large Scale Machine Learning</h3>
					<h4>Chris Lattner (Modular.ai)</h4>
					<p>
                        Chris is the co-founder and CEO of Modular AI. He cofounded the LLVM Compiler infrastructure, the Clang compiler, the Swift programming language, the MLIR compiler infrastructure, the CIRCT project (applying MLIR to hardware design), and have contributed to many other commercial and open source projects at Apple, Tesla, Google, and SiFive. Previously, Chris led the Engineering and Product teams at SiFive, the Google TensorFlow team, the Tesla Autopilot team, and worked for Apple managing the Developer Tools department.
					</p>
				</div>
        </div>
    	<div class="speaker-bio">
				<div>
					<h3 class="talk-speaker">Productionizing Large Language Models</h3>
					<h4>Ben Mann (Anthropic)</h4>
					<p>
					    Ben Mann is a co-founder at Anthropic, currently leading the product engineering team. Previously, he was a software engineering manager at Google and one of the first authors on the GPT-3 paper and the scaling law paper at OpenAI.
					</p>
				</div>
        </div>
    	<div class="speaker-bio">
				<div>
					<h3 class="talk-speaker">Challenges and Opportunities in Using ML for Training LLMs</h3>
					<h4>Susan Zhang (ex-Meta, ex-OpenAI)</h4>
					<p>
                    Susan Zhang is an AI research engineer focused on pushing the limits of compute for general pre-training methods, along with infrastructure design for AI experimentation at scale.  She has over 10 years of experience building software systems tackling a wide variety of domains, ranging from quantum computing at Los Alamos to photonic AI hardware design at Luminous Computing.  She graduated with an undergraduate degree in mathematics from Princeton University, and she is hoping to see the day when machines are the ones who finally solve the six remaining Millennium Prize problems.
					</p>
				</div>
        </div>
        <div class="speaker-bio">
				<div>
					<h3 class="talk-speaker">Multi-Agent Reinforcement Learning for Computer Architecture</h3>
					<h4>Natasha Jaques (University of Washington, Google DeepMind)</h4>
					<p>Natasha is an incoming assistant professor at the University of Washington School of Computer Science. She is currently a Senior Research Scientist at Google Brain. From 2020-2022, Natasha held this position jointly with a Visiting Postdoctoral Scholar position at UC Berkeley, in Sergey Levine’s group. Before that, Natasha received her PhD from MIT, where she worked on Affective Computing and deep/reinforcement/machine learning in Rosalind Picard’s group. Natasha has interned at DeepMind, Google Brain, and worked as an OpenAI Scholars mentor.
					</p>
				</div>
        </div>

<div class="contact-us-section">
    <div class="inner clearfix">
        <section class="main-content">
            <h2>Contact Us</h2>
            <p>
                Contact us at <a href="mailto:mlforsystems@googlegroups.com">mlforsystems@googlegroups.com</a>.
            </p>
        </section>
    </div>
</div>
